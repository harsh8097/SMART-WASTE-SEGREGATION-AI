{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LFp-T82c4MkZQVKxLTr4png8cADbFPqH",
      "authorship_tag": "ABX9TyNU9Sd1ty2wzFAkp8D9Pygf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh8097/SMART-WASTE-SEGREGATION-AI/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA5cNs7eKsE-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0M_t1Qr7c4DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/archive (1).zip\"   # folder containing plastic/metal/organic/e-waste\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n"
      ],
      "metadata": {
        "id": "sKAsTClxLqx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the zip file. Ensure it points to the original zip archive.\n",
        "zip_file_path = \"/content/archive (1).zip\"\n",
        "\n",
        "# Define the directory to extract the contents into\n",
        "extracted_dir_name = 'waste_data'\n",
        "extracted_path = os.path.join('/content', extracted_dir_name)\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extracted_path):\n",
        "    os.makedirs(extracted_path)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# Update DATASET_PATH to point to the actual directory containing class subfolders\n",
        "# Assuming the structure is /content/waste_data/dataset-resized/class1/image.jpg\n",
        "DATASET_PATH = os.path.join(extracted_path, 'dataset-resized')\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "class_names = list(train_data.class_indices.keys())\n",
        "print(\"Classes:\", class_names)\n"
      ],
      "metadata": {
        "id": "ma6txavkLyIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(\n",
        "    input_shape=(224,224,3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n"
      ],
      "metadata": {
        "id": "gjiO6ETFL4l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "6MfUU68FMPtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "W3AJXDHcMSn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eGnGIr5HMWjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save(\"waste_classifier.h5\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "WT-ENCICMYpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"waste_classifier.h5\")\n",
        "\n",
        "# Optional compile (removes warning)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "gRw2lWXoMaeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_waste(img_path):\n",
        "    img = image.load_img(img_path, target_size=IMAGE_SIZE)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = img_array / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    class_index = np.argmax(prediction)\n",
        "    confidence = np.max(prediction) * 100\n",
        "\n",
        "    print(f\"Predicted Waste: {class_names[class_index]}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lvgEGjdMMcZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_waste(\"/content/waste_data/dataset-resized/plastic/plastic102.jpg\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4HF_LR-bMhF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit tensorflow opencv-python pillow numpy\n"
      ],
      "metadata": {
        "id": "_mfIR_uTV7Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# ---------------- LOAD MODEL ----------------\n",
        "model = load_model(\"waste_classifier.h5\")\n",
        "\n",
        "# Load class labels (make sure order matches training)\n",
        "class_names = [\"plastic\", \"metal\", \"organic\", \"e-waste\"]\n",
        "\n",
        "# ---------------- PREDICTION FUNCTION ----------------\n",
        "def predict_waste_gradio(img):\n",
        "    # Convert image to RGB\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize(IMAGE_SIZE)\n",
        "\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Model prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    index = np.argmax(predictions)\n",
        "    confidence = predictions[0][index] * 100\n",
        "\n",
        "    return f\"{class_names[index]} ({confidence:.2f}%)\"\n",
        "\n",
        "# ---------------- GRADIO INTERFACE ----------------\n",
        "title = \"‚ôªÔ∏è Smart Waste Segregation AI\"\n",
        "description = \"Upload an image of waste and the AI will classify it as Plastic, Metal, Organic, or E-Waste\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predict_waste_gradio,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Label(num_top_classes=1),\n",
        "    title=title,\n",
        "    description=description,\n",
        "    examples=None\n",
        ")\n",
        "\n",
        "# ---------------- LAUNCH ----------------\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "bialwxHCZNfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daaef315"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "st.set_page_config(page_title=\"Smart Waste Segregation AI\", layout=\"centered\")\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# ---------------- LOAD MODEL ----------------\n",
        "model = load_model(\"waste_classifier.h5\")\n",
        "\n",
        "# The class_names variable is already defined in a previous cell (ma6txavkLyIu)\n",
        "# We'll assume class_names is globally available if this is run in a Colab session\n",
        "# For standalone execution, you'd need to define it or load it from somewhere.\n",
        "\n",
        "# In a typical Colab environment, 'class_names' will be available from previous execution\n",
        "# If running as a standalone script, you'd need to ensure 'class_names' is defined.\n",
        "# For now, let's assume it's accessible.\n",
        "# Example for standalone: class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "\n",
        "# ---------------- PREDICTION FUNCTION ----------------\n",
        "def predict_image(img):\n",
        "    img = img.resize(IMAGE_SIZE)\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    index = np.argmax(predictions)\n",
        "    confidence = predictions[0][index] * 100\n",
        "\n",
        "    # Access the global class_names variable\n",
        "    global class_names\n",
        "    if 'class_names' not in globals():\n",
        "        # Fallback if class_names isn't globally defined (e.g., if running app.py directly)\n",
        "        # This would ideally be loaded from a config or passed in.\n",
        "        # For this Colab context, it should be defined.\n",
        "        st.error(\"class_names not found. Please ensure the data loading cell ran.\")\n",
        "        return \"Unknown\", 0.0\n",
        "\n",
        "    return class_names[index], confidence\n",
        "\n",
        "# ---------------- UI ----------------\n",
        "st.title(\"‚ôªÔ∏è Smart Waste Segregation AI\")\n",
        "st.markdown(\"Upload an image or use live camera to classify waste\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"üì§ Upload Image\", \"üì∑ Live Camera\"])\n",
        "\n",
        "# ---------------- IMAGE UPLOAD ----------------\n",
        "with tab1:\n",
        "    uploaded_file = st.file_uploader(\"Upload waste image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        if st.button(\"Predict\"):\n",
        "            label, confidence = predict_image(image)\n",
        "            st.success(f\"üóëÔ∏è Waste Type: **{label}**\")\n",
        "            st.info(f\"üîç Confidence: **{confidence:.2f}%**\")\n",
        "\n",
        "# ---------------- LIVE CAMERA ----------------\n",
        "with tab2:\n",
        "    st.warning(\"Live camera functionality might not work directly in all Colab environments due to browser security policies.\")\n",
        "\n",
        "    start_camera = st.checkbox(\"Start Camera\")\n",
        "\n",
        "    if start_camera:\n",
        "        st.error(\"Live camera is not directly supported in Colab's Streamlit deployments without advanced setup (e.g., using a webRTC component). Please use the 'Upload Image' tab.\")\n",
        "        # The following code block is for local execution only and will not work in Colab's hosted environment\n",
        "        # cap = cv2.VideoCapture(0)\n",
        "        # stframe = st.empty()\n",
        "\n",
        "        # while cap.isOpened():\n",
        "        #     ret, frame = cap.read()\n",
        "        #     if not ret:\n",
        "        #         break\n",
        "\n",
        "        #     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        #     img = Image.fromarray(frame_rgb)\n",
        "\n",
        "        #     label, confidence = predict_image(img)\n",
        "\n",
        "        #     cv2.putText(\n",
        "        #         frame,\n",
        "        #         f\"{label} ({confidence:.1f}%)\",\n",
        "        #         (10, 40),\n",
        "        #         cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        #         1,\n",
        "        #         (0, 255, 0),\n",
        "        #         2\n",
        "        #     )\n",
        "\n",
        "        #     stframe.image(frame, channels=\"BGR\")\n",
        "\n",
        "        # cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('bash -c \"npx localtunnel --port 8501 & streamlit run app.py --server.port 8501 --server.enableCORS False --server.enableXsrfProtection False\"')\n",
        "\n"
      ],
      "metadata": {
        "id": "zpjE9zKRYnGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}